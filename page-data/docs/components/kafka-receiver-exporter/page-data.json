{
    "componentChunkName": "component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-docs-query-js",
    "path": "/docs/components/kafka-receiver-exporter",
    "result": {"data":{"mdx":{"id":"451da6a7-53f1-5e2e-827c-dfd09f1ce789","excerpt":"The Kafka receiver and exporter allows you to send and receive telemetry signals (currently traces and metrics) from a Kafka cluster. The exporter will\nproduceâ€¦","fields":{"slug":"/docs/components/kafka-receiver-exporter/"},"frontmatter":{"title":"Kafka receiver and exporter","description":"There are two components that allows you to use Kafka as a transport mechanism for telemetry signals:\nThe Kafka receiver allows the collector to receive telemetry signals from Kafka while the Kafka exporter allows you to send them to Kafka.\n","image":null,"disableTableOfContents":null},"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Kafka receiver and exporter\",\n  \"description\": \"There are two components that allows you to use Kafka as a transport mechanism for telemetry signals:\\nThe Kafka receiver allows the collector to receive telemetry signals from Kafka while the Kafka exporter allows you to send them to Kafka.\\n\",\n  \"path\": \"/docs/components/kafka-receiver-exporter\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"The Kafka receiver and exporter allows you to send and receive telemetry signals (currently traces and metrics) from a Kafka cluster. The exporter will\\nproduce telemetry signals and will submit it to a configured topic. A Kafka receiver can consume from this topic and further send\\ndata to the telemetry pipeline configured in this collector instance. This component is flexible enough that you can have\\nmultiple consumer groups for the same topic.\"), mdx(SectionSeparator, {\n    mdxType: \"SectionSeparator\"\n  }), mdx(\"h2\", {\n    \"id\": \"upstream-kafka-exporterreceiver-documentation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#upstream-kafka-exporterreceiver-documentation\",\n    \"aria-label\": \"upstream kafka exporterreceiver documentation permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  }))), \"Upstream Kafka Exporter/Receiver documentation\"), mdx(\"p\", null, \"Please find bellow the documentation for each of these components:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/kafkaexporter/README.md\"\n  }, \"Kafka Exporter\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkareceiver/README.md\"\n  }, \"Kafka Receiver\"))), mdx(\"p\", null, \"Notes:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"These components don't create Kafka topics on your behalf. They need to be pre-created or you need to set the\\ncluster configuration to \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://kafka.apache.org/documentation/#brokerconfigs_auto.create.topics.enable\"\n  }, \"auto create topics\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"It is a good practice to set the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"protocol_version\"), \" property to match the version of Kafka used in your cluster.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The kafka exporter uses a synchronous producer that blocks and does not batch messages, therefore it should be used with batch and queued retry processors for higher throughput and resiliency.\")), mdx(\"p\", null, \"The ADOT collector is tested with Kafka Versions 2.8.1 and 3.2.0.\"), mdx(\"h2\", {\n    \"id\": \"integrating-with-amazon-managed-streaming-for-apache-kafka-msk\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#integrating-with-amazon-managed-streaming-for-apache-kafka-msk\",\n    \"aria-label\": \"integrating with amazon managed streaming for apache kafka msk permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  }))), \"Integrating with Amazon Managed Streaming for Apache Kafka (MSK)\"), mdx(\"p\", null, \"The Kafka receiver and exporters can be integrated with \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://aws.amazon.com/msk/\"\n  }, \"Amazon Managed Streaming for Apache Kafka (MSK)\"), \".\"), mdx(\"p\", null, \"You can create a MSK cluster following \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.aws.amazon.com/msk/latest/developerguide/getting-started.html\"\n  }, \"this guide\"), \".\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Select the authentication method accordingly to the options supported by the receivers/exporters.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Enable TLS.\")), mdx(\"p\", null, \"After the cluster is provisioned, you can get the list of brokers by selecting the newly created cluster in the console and then\\nclicking in the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"View client information\"), \" button.\"), mdx(\"p\", null, \"With the list of brokers, you can configure the collector accordingly.\"), mdx(\"p\", null, \"For a Kafka receiver the configuration would look like:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"  kafka/receiver:\\n    auth:\\n      tls:\\n        insecure: false\\n    protocol_version: 3.2.0\\n    topic: some-topic\\n    brokers:\\n      - b-2.testcluster.abc123.c13.kafka.us-west-2.amazonaws.com:9094\\n      - b-1.testcluster.abc123.c13.kafka.us-west-2.amazonaws.com:9094\\n      - b-3.testcluster.abc123.c13.kafka.us-west-2.amazonaws.com:9094\\n\")), mdx(\"p\", null, \"For a Kafka exporter, the configuration would look like:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"  kafka/exporter:\\n    auth:\\n      tls:\\n        insecure: false\\n    protocol_version: 3.2.0\\n    topic: some-topic\\n    brokers:\\n      - b-2.testcluster.abc123.c13.kafka.us-west-2.amazonaws.com:9094\\n      - b-1.testcluster.abc123.c13.kafka.us-west-2.amazonaws.com:9094\\n      - b-3.testcluster.abc123.c13.kafka.us-west-2.amazonaws.com:9094\\n\")), mdx(\"p\", null, \"With such configuration you decouple producers (collector with exporter) and consumers (collector with receiver). This allow you to scale out consumers in case they perform some kind of processing on the telemetry signals as well as change the destination backend in the consumers without downtime as telemetry signals will still be recorded into the kafka topics while consumers are having their configuration changed.\"), mdx(\"h3\", {\n    \"id\": \"example\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#example\",\n    \"aria-label\": \"example permalink\",\n    \"className\": \"anchor before\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  }))), \"Example\"), mdx(\"p\", null, \"In this section we are going to describe the simplest possible example to integrate the kafka receiver with the kafka exporter. We will be using two collector instances: A and B. Those instances are running in two different ec2 hosts.\"), mdx(\"p\", null, \"We are going to send telemetry data to collector A, which will submit this data to a Kafka topic. The collector B will be consuming from this same topic and it will receive this telemetry.\"), mdx(\"p\", null, \"Configuration for instance A.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"receivers:\\n  statsd:\\n    endpoint: 0.0.0.0:4567\\n    aggregation_interval: 5s\\n\\nexporters:\\n  kafka/exporter:\\n    protocol_version: \\\"${extra_data.msk.kafka_version}\\\"\\n    auth:\\n      tls:\\n        insecure: false\\n    topic: adot-collector-test\\n    brokers:\\n      - b-3.aocmskcluster281.test.c13.kafka.us-west-2.amazonaws.com:9094\\n      - b-1.aocmskcluster281.test.c13.kafka.us-west-2.amazonaws.com:9094\\n      - b-2.aocmskcluster281.test.c13.kafka.us-west-2.amazonaws.com:9094\\n\\nservice:\\n  pipelines:\\n    metrics:\\n      receivers: [statsd]\\n      exporters: [kafka/exporter]\\n\")), mdx(\"p\", null, \"Configuration for instance B.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"receivers:\\n  kafka/receiver:\\n    topic: adot-collector-test\\n    protocol_version: \\\"2.8.1\\\"\\n    auth:\\n      tls:\\n        insecure: false\\n    brokers:\\n      - b-3.aocmskcluster281.test.c13.kafka.us-west-2.amazonaws.com:9094\\n      - b-1.aocmskcluster281.test.c13.kafka.us-west-2.amazonaws.com:9094\\n      - b-2.aocmskcluster281.test.c13.kafka.us-west-2.amazonaws.com:9094\\n\\nexporters:\\n  debug:\\n    verbosity: detailed\\n\\nservice:\\n  pipelines:\\n    metrics:\\n      receivers: [kafka/receiver]\\n      exporters: [debug]\\n\")), mdx(\"p\", null, \"In the ec2 instance of the collector A we execute the following command to send telemetry to the statsd receiver:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"echo \\\"kafka.test.example:1|c\\\" | nc -w 1 -u localhost 4567\\n\")), mdx(\"p\", null, \"The following output is obtained in the collector B.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"{\\\"level\\\":\\\"info\\\",\\\"timestamp\\\":\\\"2023-04-05T03:56:23.531Z\\\",\\\"message\\\":\\\"ResourceMetrics #0\\\\nResource SchemaURL: \\\\nScopeMetrics #0\\\\nScopeMetrics SchemaURL: \\\\nInstrumentationScope  \\\\nMetric #0\\\\nDescriptor:\\\\n     -> Name: kafka.test.example\\\\n     -> Description: \\\\n     -> Unit: \\\\n     -> DataType: Sum\\\\n     -> IsMonotonic: false\\\\n     -> AggregationTemporality: Delta\\\\nNumberDataPoints #0\\\\nStartTimestamp: 2023-04-05 03:56:18.402491427 +0000 UTC\\\\nTimestamp: 2023-04-05 03:56:23.401572412 +0000 UTC\\\\nValue: 1\\\\n\\\",\\\"kind\\\":\\\"exporter\\\",\\\"data_type\\\":\\\"metrics\\\",\\\"name\\\":\\\"debug\\\"}\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","headings":[{"depth":2,"value":"Upstream Kafka Exporter/Receiver documentation"},{"depth":2,"value":"Integrating with Amazon Managed Streaming for Apache Kafka (MSK)"},{"depth":3,"value":"Example"}]}},"pageContext":{"slug":"/docs/components/kafka-receiver-exporter/","repositoryEditUrl":"https://github.com/rocketseat/gatsby-themes/tree/main/examples/gatsby-theme-docs/src/docs/components/kafka-receiver-exporter.mdx","repositoryProvider":"GitHub"}},
    "staticQueryHashes": ["2501019404","973074209"]}